{"cells":[{"metadata":{"id":"ry9pxBnkYN84","colab_type":"text"},"cell_type":"markdown","source":"# vX2\n## v200122\n* Done: cleaning StandardScaler\n* Error: weighting unbalanced train set with SMOTE (library does not work on Kaggle)\n* Fixed train by removing last assessments data\n* LightGBM with tresholds"},{"metadata":{"id":"DkVjyD_iYIZq","colab_type":"text"},"cell_type":"markdown","source":"# **Accessing working environment Kaggle**"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"U5JPCJRdV1Js","colab_type":"code","colab":{}},"cell_type":"code","source":"# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"id":"c-2hWk71V1Jw","colab_type":"text"},"cell_type":"markdown","source":"# **Importing libraries**"},{"metadata":{"trusted":true,"id":"PNt-wwSRV1Jx","colab_type":"code","colab":{}},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\npd.set_option('display.max_columns', 200)\nfrom time import time\nimport datetime as dt\nimport gc # clear garbage","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Debugging f-ions"},{"metadata":{"trusted":true},"cell_type":"code","source":"def debugging_ids(df):\n    return print(f'Debugging submitted dataframe: \\nUnique installation_ids: {len(set(df.installation_id))} \\nRows & columns count {df.shape}')","execution_count":null,"outputs":[]},{"metadata":{"id":"C7YAu-l0V1J0","colab_type":"text"},"cell_type":"markdown","source":"# **Loading data**"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"id":"uC5uoz8KV1J1","colab_type":"code","colab":{}},"cell_type":"code","source":"load_columns = ['event_id',\n                'game_session',\n                'timestamp',                \n                'installation_id',\n                'event_count',\n                'event_code',\n                'game_time',\n                'title',\n                'type',\n                'world',\n                'event_data']\n\npath = '/kaggle/input/data-science-bowl-2019/' # create url path to the datasets\n\nt0 = time()\n\nprint('Loading datasets...')\nX_train = pd.read_csv(path + 'train.csv', usecols = load_columns)\nX_labels = pd.read_csv(path + 'train_labels.csv')\n# specs = pd.read_csv(path + 'specs.csv')\n#X_test = pd.read_csv(path + 'test.csv', usecols = load_columns)\n#submission = pd.read_csv(path + 'sample_submission.csv')\nprint(\"Datasets loaded successfully! \\nLoading time:\", round(time() - t0, 3), \"s\")","execution_count":null,"outputs":[]},{"metadata":{"id":"jH6Z3N8dV1J6","colab_type":"text"},"cell_type":"markdown","source":"# **Data preparation**"},{"metadata":{"id":"dng7_BgMV1J6","colab_type":"text"},"cell_type":"markdown","source":"### **(T) Reducing train df with users having accuracy scores (17000 -> 3614 installation_ids)**"},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train has 17000 installation_id's, however there are only for 3614 installation_id's (X_labels and X_train) with Assessment attempt\n# Reducing X_train to 17000 -> 3614 installation_ids\nX_train = X_train[X_train['installation_id'].isin(set(X_labels.installation_id))]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Extracting accuracy of previous Assessment attempts**\n\n* Preparing train set which is identical to train_labels except:\n* accuracy differs for 46 observations due to saving in more floating points (16 ours vs 9 train_labels.csv)\n* removed the last assessment's (target) row"},{"metadata":{},"cell_type":"markdown","source":"#### (T) Create X_train_gt by extracting only rows with assessments events"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating X_train_gt to hold only rows with assessment attempts\n\nX_train_gt = pd.DataFrame(data=None)\n\n# X_train_gt will be used only for accuracy features extraction\n# First, filter assessment events only\n\nX_train_gt = X_train[((X_train['event_code'] == 4100) & \n                 (X_train['title'].isin(['Cart Balancer (Assessment)', \n                                    'Cauldron Filler (Assessment)', \n                                    'Mushroom Sorter (Assessment)', \n                                    'Chest Sorter (Assessment)']))) | \n                ((X_train['event_code'] == 4110) & \n                 (X_train['title'] == 'Bird Measurer (Assessment)'))].copy(deep=True)   ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# debugging\ndebugging_ids(X_train_gt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_train_gt[X_train_gt['installation_id'] == '0006c192']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### (T) Drop columns which will be processed later"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fourth, drop columns which will be processed separately\n\nX_train_gt.drop(['event_id', \n                 'timestamp', \n                 'event_count', \n                 'event_code', \n                 'game_time',\n                 'type',\n                 'world',], axis=1, inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# debugging\ndebugging_ids(X_train_gt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### (T) Extract accuracy features from 'event_data'"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fifths, extract correct and incorrect assessment attempts per user from 'event_data'\n# Create num_correct and num_incorrect columns\n\ncorr = '\"correct\":true'\nincorr = '\"correct\":false'\n\nX_train_gt['num_correct'] = X_train_gt['event_data'].apply(lambda x: 1 if corr in x else 0)\nX_train_gt['num_incorrect'] = X_train_gt['event_data'].apply(lambda x: 1 if incorr in x else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# debugging\ndebugging_ids(X_train_gt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_train_gt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sixths, aggregate (sum) correct and incorrect assessment attempts \n# per 'game_session', 'installation_id' and assessment 'title'\n# As provided in grount truth (labels.csv)\n\n# previous aggregation was made together with sorting to match train_labels format\n#X_train_gt = X_train_gt.sort_values(['installation_id', 'game_session'], ascending=True).groupby(['game_session', 'installation_id', 'title'], as_index=False, sort=False).agg(sum)\n# a) difficult to extract last assessment\n# b) difficult to truncate\n# c) difficult to accumulate actions before assessment\nX_train_gt = X_train_gt.groupby(['game_session', 'installation_id', 'title'], as_index=False, sort=False).agg(sum)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_train_gt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Great, because w/o sorting by game_session and installation_id \n# # we preserve the original order of events by timestamp \n# X_train_gt[X_train_gt['installation_id'] == '0006c192']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_labels[X_labels['installation_id'] == '0006c192']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_train[(X_train['installation_id'] == '0006c192') & ((X_train['event_code'] == 4100) | (X_train['event_code'] == 4110))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Sevenths, create 'accuracy' feature = corr / (corre + incorr)\n\nX_train_gt['accuracy'] = X_train_gt['num_correct'] / (X_train_gt['num_correct'] + X_train_gt['num_incorrect'])\n\n# Eighths, create 'accuracy_group' feature\n# 3: the assessment was solved on the first attempt\n# 2: the assessment was solved on the second attempt\n# 1: the assessment was solved after 3 or more attempts\n# 0: the assessment was never solved\n\n# If accuracy is 0.0 (no correct attempts), accuracy group is 0 as all observations in X_train_gt by now has at least one attempt\n# If accuracy is 1.0 (that is no incorrect attempts), accuracy group is 3\n# If accuracy is 0.5 (there is equal amount of correct and incorrect attempts), accuracy group is 2\n# Any other case means that accuracy group equals 1, that is 3 or more attempts were needed to make a correct attempt    \n\nX_train_gt['accuracy_group'] = X_train_gt['accuracy'].apply(lambda x: 0 if x == 0.0 else (3 if x == 1.0 else (2 if x == 0.5 else 1)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# debugging\ndebugging_ids(X_train_gt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # task is to forecast 'accuracy_group' in the last 'game_session' of single 'installation_id'\n# # E.g. 'installation_id' '0006a69f' last assessment\n# # in last 'game_session' 'a9ef3ecb3d1acc6a' was 'Bird Measurer (Assessment)'\n# # our task is to forecast that his 'accuracy_group' was '3' \n# X_train_gt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Comparing with ground truth sample:\n# X_labels.head(8)\n# # As we removed sorting, only overall count should match","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# # Double check accuracy figures in X_train_gt and X_labels\n\n# print(f'SUM (OK)')\n# print(f'X_train_gt has accuracy_group sum of {sum(X_train_gt[\"accuracy_group\"])} \\nX_labels has accuracy_group sum of {sum(X_labels[\"accuracy_group\"])}')\n\n# print(f'\\nTYPE (OK)')\n# print(f'Type of X_train_gt num_correct is {type(X_train_gt[\"num_correct\"][0])} \\nType of X_labels num_correct is {type(X_labels[\"num_correct\"][0])}')\n# print(f'Type of X_train_gt num_incorrect is {type(X_train_gt[\"num_incorrect\"][0])} \\nType of X_labels num_incorrect is {type(X_labels[\"num_incorrect\"][0])}') \n# print(f'Type of X_train_gt accuracy is {type(X_train_gt[\"accuracy\"][0])} \\nType of X_labels accuracy is {type(X_labels[\"accuracy\"][0])}') \n# print(f'Type of X_train_gt accuracy_group is {type(X_train_gt[\"accuracy_group\"][0])} \\nType of X_labels accuracy_group is {type(X_labels[\"accuracy_group\"][0])}')\n\n# print(f'\\nDIFFERENCES')\n# print(f'Difference between accuracy column in X_train_gt and X_labels is: {set(X_train_gt[\"accuracy\"] - X_labels[\"accuracy\"])}')\n# print(f'Difference between accuracy_group column in X_train_gt and X_labels is: {set(X_train_gt[\"accuracy_group\"] - X_labels[\"accuracy_group\"])}')\n# print(f'Accuracy set len in X_train_gt is: {len(set(X_train_gt[\"accuracy\"]))}')\n# print(f'Accuracy set len in X_labels is: {len(set(X_labels[\"accuracy\"]))}')\n# print(f'Difference between num_correct column in X_train_gt and X_labels is: {set(X_train_gt[\"num_correct\"] - X_labels[\"num_correct\"])}')\n# print(f'Difference between num_incorrect column in X_train_gt and X_labels is: {set(X_train_gt[\"num_incorrect\"] - X_labels[\"num_incorrect\"])}')\n\n# print(f'\\nEQUAL VALUES ROW BY ROW')\n\n# booltest_session = X_train_gt.game_session == X_labels.game_session\n# booltest_ids = X_train_gt.installation_id == X_labels.installation_id\n# booltest_title = X_train_gt.title == X_labels.title\n# booltest_num_correct = X_train_gt.num_correct == X_labels.num_correct\n# booltest_num_incorrect = X_train_gt.num_incorrect == X_labels.num_incorrect\n# booltest_accuracy = X_train_gt.accuracy == X_labels.accuracy\n# booltest_accuracy_group = X_train_gt.accuracy_group == X_labels.accuracy_group\n\n# print(f'Equal values (TRUE) of game_session in X_train_gt and X_labels: \\n{booltest_session.value_counts()}')\n# print(f'Equal values (TRUE) of installation_id in X_train_gt and X_labels: \\n{booltest_ids.value_counts()}')\n# print(f'Equal values (TRUE) of title in X_train_gt and X_labels: \\n{booltest_title.value_counts()}')\n# print(f'Equal values (TRUE) of num_correct in X_train_gt and X_labels: \\n{booltest_num_correct.value_counts()}')\n# print(f'Equal values (TRUE) of num_incorrect in X_train_gt and X_labels: \\n{booltest_num_incorrect.value_counts()}')\n# print(f'Equal values (TRUE) of accuracy in X_train_gt and X_labels: \\n{booltest_accuracy.value_counts()}')\n# print(f'Equal values (TRUE) of accuracy_group in X_train_gt and X_labels: \\n{booltest_accuracy_group.value_counts()}')\n\n# # Changelog:\n# # Index was fixed by applying .sort_values(['installation_id', 'game_session'], ascending=True) in the groupby part\n# # Now difference between accuracy_group columns in X_train_gt and X_labels should be {0}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"## Debugging 46 accuracy scores which do not match.\n# not_matching_accuracy_df = X_train_gt.accuracy - X_labels.accuracy\n# not_matching_accuracy_df = not_matching_accuracy_df[not_matching_accuracy_df != 0]\n# #len(not_matching_accuracy_df) = 46\n# X_train_gt[X_train_gt.index.isin(not_matching_accuracy_df.index)]\n# # X_labels[X_labels.index.isin(not_matching_accuracy_df.index)]\n# # Conclusion: We produce 16 digits after comma, train_labels.csv has 9\n#X_train_gt[X_train_gt.index.isin(not_matching_accuracy_df.index)].to_csv(\"different_accuracies.csv\", index = False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### (T) Accuracy groups"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"X_train_gt['acc_0'] = X_train_gt['accuracy_group'].apply(lambda x: 1 if x == 0 else 0)\nX_train_gt['acc_1'] = X_train_gt['accuracy_group'].apply(lambda x: 1 if x == 1 else 0)\nX_train_gt['acc_2'] = X_train_gt['accuracy_group'].apply(lambda x: 1 if x == 2 else 0)\nX_train_gt['acc_3'] = X_train_gt['accuracy_group'].apply(lambda x: 1 if x == 3 else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# debugging\n# X_train_gt[X_train_gt['installation_id'] == '0006a69f']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# debugging\ndebugging_ids(X_train_gt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### (T) Accuracy groups per assessment 'title'"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# Accuracy group per assessment title\n# Ref: https://stackoverflow.com/questions/27474921/compare-two-columns-using-pandas/27475029\n# (condition, output value, else)\n\nX_train_gt['bird_accg_0'] = np.where((X_train_gt['title'] == 'Bird Measurer (Assessment)') & (X_train_gt['accuracy_group'] == 0), 1, 0)\nX_train_gt['bird_accg_1'] = np.where((X_train_gt['title'] == 'Bird Measurer (Assessment)') & (X_train_gt['accuracy_group'] == 1), 1, 0)\nX_train_gt['bird_accg_2'] = np.where((X_train_gt['title'] == 'Bird Measurer (Assessment)') & (X_train_gt['accuracy_group'] == 2), 1, 0)\nX_train_gt['bird_accg_3'] = np.where((X_train_gt['title'] == 'Bird Measurer (Assessment)') & (X_train_gt['accuracy_group'] == 3), 1, 0)\n\nX_train_gt['cart_accg_0'] = np.where((X_train_gt['title'] == 'Cart Balancer (Assessment)') & (X_train_gt['accuracy_group'] == 0), 1, 0)\nX_train_gt['cart_accg_1'] = np.where((X_train_gt['title'] == 'Cart Balancer (Assessment)') & (X_train_gt['accuracy_group'] == 1), 1, 0)\nX_train_gt['cart_accg_2'] = np.where((X_train_gt['title'] == 'Cart Balancer (Assessment)') & (X_train_gt['accuracy_group'] == 2), 1, 0)\nX_train_gt['cart_accg_3'] = np.where((X_train_gt['title'] == 'Cart Balancer (Assessment)') & (X_train_gt['accuracy_group'] == 3), 1, 0)\n\nX_train_gt['cauldron_accg_0'] = np.where((X_train_gt['title'] == 'Cauldron Filler (Assessment)') & (X_train_gt['accuracy_group'] == 0), 1, 0)\nX_train_gt['cauldron_accg_1'] = np.where((X_train_gt['title'] == 'Cauldron Filler (Assessment)') & (X_train_gt['accuracy_group'] == 1), 1, 0)\nX_train_gt['cauldron_accg_2'] = np.where((X_train_gt['title'] == 'Cauldron Filler (Assessment)') & (X_train_gt['accuracy_group'] == 2), 1, 0)\nX_train_gt['cauldron_accg_3'] = np.where((X_train_gt['title'] == 'Cauldron Filler (Assessment)') & (X_train_gt['accuracy_group'] == 3), 1, 0)\n\nX_train_gt['chest_accg_0'] = np.where((X_train_gt['title'] == 'Chest Sorter (Assessment)') & (X_train_gt['accuracy_group'] == 0), 1, 0)\nX_train_gt['chest_accg_1'] = np.where((X_train_gt['title'] == 'Chest Sorter (Assessment)') & (X_train_gt['accuracy_group'] == 1), 1, 0)\nX_train_gt['chest_accg_2'] = np.where((X_train_gt['title'] == 'Chest Sorter (Assessment)') & (X_train_gt['accuracy_group'] == 2), 1, 0)\nX_train_gt['chest_accg_3'] = np.where((X_train_gt['title'] == 'Chest Sorter (Assessment)') & (X_train_gt['accuracy_group'] == 3), 1, 0)\n\nX_train_gt['mushroom_accg_0'] = np.where((X_train_gt['title'] == 'Mushroom Sorter (Assessment)') & (X_train_gt['accuracy_group'] == 0), 1, 0)\nX_train_gt['mushroom_accg_1'] = np.where((X_train_gt['title'] == 'Mushroom Sorter (Assessment)') & (X_train_gt['accuracy_group'] == 1), 1, 0)\nX_train_gt['mushroom_accg_2'] = np.where((X_train_gt['title'] == 'Mushroom Sorter (Assessment)') & (X_train_gt['accuracy_group'] == 2), 1, 0)\nX_train_gt['mushroom_accg_3'] = np.where((X_train_gt['title'] == 'Mushroom Sorter (Assessment)') & (X_train_gt['accuracy_group'] == 3), 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# debugging\n#X_train_gt['mushroom_accg_0'][17688]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# debugging\n#X_train_gt[X_train_gt['installation_id'] == '0006a69f']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# debugging\ndebugging_ids(X_train_gt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### (T) Accuracy (corr, incorr, accuracy) per assessment"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Accuracy group per assessment title\n# Ref: https://stackoverflow.com/questions/27474921/compare-two-columns-using-pandas/27475029\n# (condition, output value, else)\n# E.g. if Bird Measurer has num_correct = 1, add 1, elsewise add 0\n\nX_train_gt['bird_correct'] = np.where((X_train_gt['title'] == 'Bird Measurer (Assessment)') & (X_train_gt['num_correct'] == 1), 1, 0)\nX_train_gt['bird_incorrect'] = np.where((X_train_gt['title'] == 'Bird Measurer (Assessment)') & (X_train_gt['num_incorrect'] > 0), X_train_gt['num_incorrect'], 0)\nX_train_gt['bird_accuracy'] = np.where((X_train_gt['title'] == 'Bird Measurer (Assessment)'), X_train_gt['accuracy'], 0)\n\nX_train_gt['cart_correct'] = np.where((X_train_gt['title'] == 'Cart Balancer (Assessment)') & (X_train_gt['num_correct'] == 1), 1, 0)\nX_train_gt['cart_incorrect'] = np.where((X_train_gt['title'] == 'Cart Balancer (Assessment)') & (X_train_gt['num_incorrect'] > 0), X_train_gt['num_incorrect'], 0)\nX_train_gt['cart_accuracy'] = np.where((X_train_gt['title'] == 'Cart Balancer (Assessment)'), X_train_gt['accuracy'], 0)\n\nX_train_gt['cauldron_correct'] = np.where((X_train_gt['title'] == 'Cauldron Filler (Assessment)') & (X_train_gt['num_correct'] == 1), 1, 0)\nX_train_gt['cauldron_incorrect'] = np.where((X_train_gt['title'] == 'Cauldron Filler (Assessment)') & (X_train_gt['num_incorrect'] > 0), X_train_gt['num_incorrect'], 0)\nX_train_gt['cauldron_accuracy'] = np.where((X_train_gt['title'] == 'Cauldron Filler (Assessment)'), X_train_gt['accuracy'], 0)\n\nX_train_gt['chest_correct'] = np.where((X_train_gt['title'] == 'Chest Sorter (Assessment)') & (X_train_gt['num_correct'] == 1), 1, 0)\nX_train_gt['chest_incorrect'] = np.where((X_train_gt['title'] == 'Chest Sorter (Assessment)') & (X_train_gt['num_incorrect'] > 0), X_train_gt['num_incorrect'], 0)\nX_train_gt['chest_accuracy'] = np.where((X_train_gt['title'] == 'Chest Sorter (Assessment)'), X_train_gt['accuracy'], 0)\n\nX_train_gt['mushroom_correct'] = np.where((X_train_gt['title'] == 'Mushroom Sorter (Assessment)') & (X_train_gt['num_correct'] == 1), 1, 0)\nX_train_gt['mushroom_incorrect'] = np.where((X_train_gt['title'] == 'Mushroom Sorter (Assessment)') & (X_train_gt['num_incorrect'] > 0), X_train_gt['num_incorrect'], 0)\nX_train_gt['mushroom_accuracy'] = np.where((X_train_gt['title'] == 'Mushroom Sorter (Assessment)'), X_train_gt['accuracy'], 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# debugging\n# X_train_gt[X_train_gt['installation_id'] == '0006a69f']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# debugging\ndebugging_ids(X_train_gt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Removing last assessment from train set\n\n* X_train_gt at this point has 41549 assessments\n* Can not remove just last one before aggregation "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Remove the last assessment's attempt from train (new from 200115)\n\n# Build temporary df which holds last assessment\nX_train_gt_last = X_train_gt.groupby('installation_id').tail(1).copy(deep=True)\nX_train_gt_last_index_list = list(X_train_gt_last.index)\n\n# Removing last assessment attempt from test set\n# 'installation_id's drop 3614->3021 as we have users who had just single attempt\nX_train_gt = X_train_gt.loc[~X_train_gt.index.isin(X_train_gt_last_index_list)]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# debugging\ndebugging_ids(X_train_gt_last)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # debugging\n# X_train_gt_last.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # debugging, good case of 0006c192\n# X_train[(X_train['installation_id'] == '0006c192') & ((X_train['event_code'] == 4100) | (X_train['event_code'] == 4110))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train_gt_last[(X_train_gt_last['installation_id'] == '0006c192')]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# debugging\ndebugging_ids(X_train_gt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### (~T) Aggregation\n\nTested the build, updated avoiding extra df, but haven't double-checked sample means or sums"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_gt_sum_list = ['num_correct', 'num_incorrect', \n       'bird_correct', 'bird_incorrect',\n       'cart_correct', 'cart_incorrect', 'cauldron_correct',\n       'cauldron_incorrect', 'chest_correct',\n       'chest_incorrect', 'mushroom_correct',\n       'mushroom_incorrect', 'acc_0',\n       'acc_1', 'acc_2', 'acc_3', 'bird_accg_0', 'bird_accg_1', 'bird_accg_2',\n       'bird_accg_3', 'cart_accg_0', 'cart_accg_1', 'cart_accg_2',\n       'cart_accg_3', 'cauldron_accg_0', 'cauldron_accg_1', 'cauldron_accg_2',\n       'cauldron_accg_3', 'chest_accg_0', 'chest_accg_1', 'chest_accg_2',\n       'chest_accg_3', 'mushroom_accg_0', 'mushroom_accg_1', 'mushroom_accg_2',\n       'mushroom_accg_3']\n\nX_train_gt_mean_list = ['accuracy',\n       'accuracy_group', 'bird_accuracy',\n       'cart_accuracy', 'cauldron_accuracy', 'chest_accuracy', 'mushroom_accuracy']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#len(X_train_gt_sum_list), len(X_train_gt_mean_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_gt_sum_df = X_train_gt.groupby(['installation_id'], as_index=False, sort=False)[X_train_gt_sum_list].agg(sum)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_train_gt_sum_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_gt_mean_df = X_train_gt.groupby(['installation_id'], as_index=False, sort=False)[X_train_gt_mean_list].agg('mean')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_train_gt_mean_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_train_gt_unchaged_df = X_train_gt.groupby(['installation_id'], as_index=False, sort=False)[X_train_gt_unchanged_list].last()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_gt = pd.merge(X_train_gt_sum_df, X_train_gt_mean_df, how='left', on=['installation_id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del X_train_gt_sum_df, X_train_gt_mean_df\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_train_gt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# debugging\ndebugging_ids(X_train_gt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Adding users w/o previous assessment attempts"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_features_list = X_train_gt.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(f'X_train iids: {len(set(X_train.installation_id))} \\nX_train_gt iids: {len(set(X_train_gt.installation_id))} \\nX_labels iids: {len(set(X_labels.installation_id))}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_users_wo_assessments = set(X_train.installation_id) - set(X_train_gt.installation_id)\nlen(train_users_wo_assessments)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating empty df matching test's columns"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_users_wo_assessments_df = pd.DataFrame(0, index=np.arange(len(train_users_wo_assessments)), columns=train_features_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_users_wo_assessments_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Adding 'installation_id's w/o prior assessments"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We have created installation_id column with zero values. Now will assign missing 'installation_id's:\ntrain_users_wo_assessments_df['installation_id'] = train_users_wo_assessments","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_users_wo_assessments_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Merging 'installation_id's with and w/o assessments"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_gt = X_train_gt.append(train_users_wo_assessments_df, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_gt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# debugging\ndebugging_ids(X_train_gt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # debugging\n# # we lost the order of 'installation_id', but submission is sorted ascending\n# booltrain_label = X_train_gt.installation_id.sort_values(ascending=True).reset_index(drop=True) == X_labels.installation_id\n# set(booltrain_label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del train_users_wo_assessments_df\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### (T) Sorting to match order of initial train set\n* Because after merger of users with previous assessments and without we lost the initial ordering"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_gt = X_train_gt.sort_values('installation_id', ascending=True).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # debugging\n# # check if sorting of 'installation_id's matches train_labels sorting\n# # for this need to drop duplicates in X_labels as it contain 17690 rows with 'installation_id's\n# # ref: https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.drop_duplicates.html\n# # reseting index and dropping old index via reset_index(drop=True)\n# # does not lose the sorting\n# # THIS PART TO BE UNCOMMENTED:\n# X_labels_unique_installation_id = X_labels.installation_id.drop_duplicates().reset_index(drop=True)\n# booltrain_label = X_train_gt.installation_id == X_labels_unique_installation_id\n# set(booltrain_label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# del X_labels_unique_installation_id, booltrain_label\n# gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# debugging\ndebugging_ids(X_train_gt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Adding previous assessments count"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_gt['previous_assessments_count'] = X_train_gt['num_correct'] + X_train_gt['num_incorrect']\n\n# debugging\ndebugging_ids(X_train_gt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Adding 'forecasted_assessment' feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_gt.shape, X_train_gt_last.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_train_gt_last","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_train[(X_train['installation_id'] == '0006c192') & ((X_train['event_code'] == 4100) | (X_train['event_code'] == 4110))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # X_train_gt_last is taking X_train index 4137->11337808\n# train_forecasted_assessment_df = X_train_gt_last.sort_values('installation_id', ascending=True).reset_index(drop=True)\n# train_forecasted_assessment_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # check if last df had the right 'title' for forecasted assessment\n# X_labels.head(20)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # double-check sorting - OK\n# boollast_label = train_forecasted_assessment_df.installation_id == X_labels_unique_installation_id\n# set(boollast_label)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train_forecasted_assessment_df.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_train_gt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Need to reset X_train_gt_last index for boolean comparison\n# X_train_gt_last","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Debugging - double-check sorting of X_train_gt_last & X_train_gt\nX_train_gt_last = X_train_gt_last.reset_index(drop=True)\n# Above we updated the X_train_gt_last index to match 0-3613 (total of 3614)\nbooltrain_last = X_train_gt.installation_id == X_train_gt_last.installation_id\nset(booltrain_last)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del booltrain_last\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Updated index:\n# X_train_gt_last","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_gt['forecasted_assessment'] = X_train_gt_last['title'].map({'Bird Measurer (Assessment)': 0,\n                                                                            'Cart Balancer (Assessment)': 1, \n                                                                            'Cauldron Filler (Assessment)': 2, \n                                                                            'Chest Sorter (Assessment)': 3, \n                                                                            'Mushroom Sorter (Assessment)': 4})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# debugging\ndebugging_ids(X_train_gt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set(X_train_gt.forecasted_assessment), X_train_gt.forecasted_assessment.count()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# (~T) Adding non accuracy features\n### bugs:\n#### - data is not truncated after forecasted_event\n#### - we take last assessment, which might better off be random\n\nGiven that test set contains almost half of installation_ids without previous assessments, we need to add other than accuracy features for model to pick up"},{"metadata":{},"cell_type":"markdown","source":"## (~T) event_code"},{"metadata":{},"cell_type":"markdown","source":"#### Preparing event_code features"},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#X_train","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def event_code(df):\n    df = pd.get_dummies(data=df, columns=['event_code'])\n    df.columns = df.columns.str.lower()\n    df.columns = df.columns.str.replace(' |-|!|\\)|\\(', '')\n    df = df.groupby(['installation_id'], as_index=False, sort=False).agg(sum)  \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Uses ~3 GB of RAM for this operation (9->12->9)\nX_train_eventcode = X_train.filter(['installation_id', 'event_code'], axis=1)\nX_train_eventcode = event_code(X_train)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"#X_train_eventcode","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Merging event_code features to the main train set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add event_code features to the main dataframe\nX_train_gt = pd.merge(X_train_gt, X_train_eventcode, on=['installation_id'])\n# # Count nan in df for debugging purposes\n# X_train_gt.isna().sum()\n\ndel X_train_eventcode\ngc.collect()\n\n# debugging\ndebugging_ids(X_train_gt)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"trusted":true},"cell_type":"code","source":"X_train_gt","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## (~T) Title, type, world and event_code"},{"metadata":{},"cell_type":"markdown","source":"#### Preparing title, type and world features"},{"metadata":{"trusted":true},"cell_type":"code","source":"gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Uses RAM 9.1->13.8->8.7\ndef title_type_world(df):\n    df = pd.get_dummies(data=df, columns=['title', 'type', 'world'])\n    df.columns = df.columns.str.lower()\n    df.columns = df.columns.str.replace(' |-|!|\\)|\\(', '')\n    df = df.groupby(['installation_id'], as_index=False, sort=False).agg(sum) \n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create new X_train_titletypeworldfeat which holds time only title, type and world features\nX_train_titletypeworldfeat = X_train.filter(['installation_id', 'title', 'type', 'world'], axis=1)\nX_train_titletypeworldfeat = title_type_world(X_train_titletypeworldfeat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#X_train_titletypeworldfeat","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Merging title, type and world features to the main train set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add title, type and world features to the main dataframe\nX_train_gt = pd.merge(X_train_gt, X_train_titletypeworldfeat, on=['installation_id'])\n# # Count nan in df for debugging purposes\n# X_train_gt.isna().sum()\n\ndel X_train_titletypeworldfeat\ngc.collect()\n\n# debugging\ndebugging_ids(X_train_gt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## (~T) Other features\n\n* all_actions_time\n\n*     Aggregate amount (in ms) of time spent on Assessments, Activities and Games\n*     Clips do not have time spent feature\n\n* all_actions_time\n* action_duration_mean (!!!)\n* event_code_count_mean\n* number_of_sessions_nu\n* event_count_mean (!!!)"},{"metadata":{},"cell_type":"markdown","source":"###### (T) all_actions_time"},{"metadata":{"trusted":true},"cell_type":"code","source":"# nstallation_id\tgame_time\n# 0\t0006a69f\t36368\n# 1\t0006c192\t216374\n# 2\t00129856\t39701\n# 3\t001d0ed0\t38115\n# 4\t00225f67\t26517\n# ...\t...\t...\n# 3609\tff9305d7\t59417\n# 3610\tff9715db\t28408\n# 3611\tffc90c32\t43142\n# 3612\tffd2871d\t54533\n# 3613\tffeb0b1b\t71511\n\n# vs\n# installation_id\tgame_time\n# 0\t0006a69f\t36368\n# 1\t0006c192\t216374\n# 2\t00129856\t39701\n# 3\t001d0ed0\t38115\n# 4\t00225f67\t26517\n# ...\t...\t...\n# 3609\tff9305d7\t59417\n# 3610\tff9715db\t28408\n# 3611\tffc90c32\t43142\n# 3612\tffd2871d\t54533\n# 3613\tffeb0b1b\t71511\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Tested, works well, except truncation after last assessment\n# Creating all_actions_time (games, activities and assessments)\n# RAM: 8.7->8.5-8.7 GB\nfeat_gametime = X_train[X_train['type'].isin(['Assessment', 'Game', 'Activity'])]\n\n# Extracting last assessment's time\nfeat_gametime_last = feat_gametime.groupby(['installation_id', 'game_session'], as_index=False, sort=False)[['game_time', 'type']].last()\nfeat_gametime_last = feat_gametime_last[feat_gametime_last['type'] == 'Assessment'].groupby('installation_id', as_index=False, sort=False)['game_time'].last()\n\n# Finalizing the whole time\nfeat_gametime = feat_gametime.groupby(['installation_id', 'game_session'], as_index=False, sort=False)['game_time'].last()\nfeat_gametime = feat_gametime.groupby('installation_id', as_index=False, sort=False)['game_time'].sum()\n\n# Removing last assessments time which is not available in test set\nfeat_gametime['game_time'] = feat_gametime['game_time'] - feat_gametime_last['game_time']\n# Difference is correct, tested\n\n# Merging to the main train set\nX_train_gt['all_actions_time'] = feat_gametime['game_time']\n\n# Deleting\ndel feat_gametime, feat_gametime_last \ngc.collect()\n\n# debugging\ndebugging_ids(X_train_gt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### action_duration_mean"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating action_duration_mean (games, activities and assessments) (!!!)\n# RAM: 8.7->9.6->8.7 GB\nfeat_gametimemean = X_train[X_train['type'].isin(['Assessment', 'Game', 'Activity'])]\nfeat_gametimemean = feat_gametimemean.groupby(['installation_id', 'game_session'], as_index=False, sort=False)['game_time'].last()\nfeat_gametimemean = feat_gametimemean.groupby('installation_id', as_index=False, sort=False)['game_time'].mean()\n\n# Merging to the main train set\nX_train_gt['action_duration_mean'] = feat_gametimemean['game_time']\n\n# Deleting\ndel feat_gametimemean\ngc.collect()\n\n# debugging\ndebugging_ids(X_train_gt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### event_code_count_mean"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating event_code_count_mean (!!!)\n# RAM: OK, flat\nfeat_eventcodecountmean = X_train.groupby(['installation_id', 'game_session'], as_index=False, sort=False)['event_code'].count()\nfeat_eventcodecountmean = feat_eventcodecountmean.groupby('installation_id', as_index=False, sort=False)['event_code'].mean()\n\n# Merging to the main train set\nX_train_gt['event_code_count_mean'] = feat_eventcodecountmean['event_code']\n\n# Deleting\ndel feat_eventcodecountmean\ngc.collect()\n\n# debugging\ndebugging_ids(X_train_gt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### number_of_sessions_nu"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating event_code_count_mean\n# RAM: OK, flat\nfeat_numberofsessions = X_train.groupby(['installation_id'], as_index=False, sort=False)['game_session'].count()\n\n# Merging to the main train set\nX_train_gt['number_of_sessions_nu'] = feat_numberofsessions['game_session']\n\n# Deleting\ndel feat_numberofsessions\ngc.collect()\n\n# debugging\ndebugging_ids(X_train_gt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### event_count_mean"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating event_count_mean (!!!)\n# RAM: OK, flat\nfeat_eventcountmean = X_train.groupby(['installation_id', 'game_session'], as_index=False, sort=False)['event_count'].last()\nfeat_eventcountmean = feat_eventcountmean.groupby('installation_id', as_index=False, sort=False)['event_count'].mean()\n\n# Merging to the main train set\nX_train_gt['event_count_mean'] = feat_eventcountmean['event_count']\n\n# Deleting\ndel feat_eventcountmean\ngc.collect()\n\n# debugging\ndebugging_ids(X_train_gt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## (~T) timestamp"},{"metadata":{"trusted":true},"cell_type":"code","source":"# bug - taking the last even, which might be not assessment\n# could replace with mean\n\nimport datetime as dt\n\ndef timestamp_split(df):\n    df['timestamp'] = pd.to_datetime(df['timestamp']) # converting argument to pandas datetime\n#    df['year'] = df['timestamp'].dt.year # all are in 2019\n    df['month'] = (df['timestamp'].dt.month).astype(int)\n    df['day'] = (df['timestamp'].dt.day).astype(int) # returns day of the month 1-31\n    df['hour'] = (df['timestamp'].dt.hour).astype(int) \n    df['minute'] = (df['timestamp'].dt.minute).astype(int)\n#    df['second'] = df['timestamp'].dt.second # doubt it could give anything\n    df['dayofweek'] = (df['timestamp'].dt.dayofweek).astype(int) # returns day of week in 0-6 integer format\n    df['dayofyear'] = (df['timestamp'].dt.dayofyear).astype(int) # returns numeric day of year, might be useful for summer holidays\n    df['quarter'] = (df['timestamp'].dt.quarter).astype(int)\n    df['is_weekend'] = (np.where(df['dayofweek'].isin(['Sunday','Saturday']), 1, 0)).astype(int)\n    df.drop(['timestamp'], axis=1, inplace=True)\n    return df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# RAM 8.7->10->9.3\n# Create new X_train_timefeat which holds time only features  \nfeat_time = X_train.filter(['installation_id', 'timestamp'], axis=1)\n# Prepare time features from given timestamp \nfeat_time = timestamp_split(feat_time)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining as last (bug)\nfeat_time = feat_time.groupby('installation_id', as_index=False).last()\n\n# Merging to the main train set\nX_train_gt = pd.merge(X_train_gt, feat_time, on=['installation_id'])\n\n# Deleting\ndel feat_time\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del X_train, X_labels\ngc.collect()\n\n# debugging\ndebugging_ids(X_train_gt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Adding train target"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Update 200117, major bug fix\nX_train_gt['Y_target'] = X_train_gt_last['accuracy_group']\n\n# debugging\ndebugging_ids(X_train_gt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preparing X, y"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"X_train_model = X_train_gt.copy(deep=True)\n\n#del X_train_gt\n#gc.collect()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_model.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Casting categorical features to str (must in Catboost & Eli5)\n# X_train_model['forecasted_assessment'] = X_train_model['forecasted_assessment'].astype(str)\n# categorical_features = ['forecasted_assessment'] #200119\n# type(X_train_model.forecasted_assessment[3612])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Elsewise LightGBMError: Do not support special JSON characters in feature name.\nX_train_model.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_train_model.columns]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### StandardScaler"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Dropping non numeric column 'installation_id'\nX_train_model = X_train_model.drop(['installation_id'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Defining scaler\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Targets"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Setting target & features\ny = X_train_model.Y_target\nfeature_names = X_train_model.columns.drop(['Y_target'])\nX = X_train_model[feature_names]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Scaling\nX_scaled = scaler.fit_transform(X.astype(np.float64))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Resampling"},{"metadata":{"trusted":true},"cell_type":"code","source":"# import tensorflow as tf\n# # from collections import Counter\n# # from sklearn.datasets import make_classification\n# # from imblearn.over_sampling import SMOTE # doctest: +NORMALIZE_WHITESPACE\n# from imblearn import undersampling, oversampling\n# from imblearn import under_sampling \n# from imblearn import over_sampling\n# from imblearn.over_sampling import SMOTE","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Ref: https://imbalanced-learn.readthedocs.io/en/stable/generated/imblearn.over_sampling.SMOTE.html\n# # from collections import Counter\n# # from sklearn.datasets import make_classification\n# from imblearn.over_sampling import SMOTE\n# # X, y = make_classification(n_classes=2, \n# #                            class_sep=2,\n# #                            weights=[0.1, 0.9], \n# #                            n_informative=3, \n# #                            n_redundant=1, \n# #                            flip_y=0,\n# #                            n_features=20, \n# #                            n_clusters_per_class=1, \n# #                            n_samples=1000, \n# #                            random_state=10)\n\n# # print('Original dataset shape %s' % Counter(y))\n\n# sm = SMOTE(random_state=42)\n# X_res, y_res = sm.fit_resample(X_scaled, y)\n# print('Resampled dataset shape %s' % Counter(y_res))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Metric"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check Cohen Kappa Score:\nfrom sklearn.metrics import cohen_kappa_score","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model w XGBoost"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from sklearn.model_selection import train_test_split\n# train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# from sklearn.metrics import accuracy_score\n# import xgboost as xgb\n\n# xgb_clf = xgb.XGBClassifier(learning_rate=0.5,\n#                     n_estimators=2000,\n#                     max_depth=6,\n#                     min_child_weight=0,\n#                     gamma=0,\n#                     reg_lambda=1,\n#                     subsample=1,\n#                     colsample_bytree=0.75,\n#                     scale_pos_weight=1,\n#                     objective='multi:softprob',\n#                     num_class=4,\n#                     verbose=200,\n#                     random_state=42,\n#                     early_stopping_rounds=10,\n#                     verbose_eval=True)\n\n# xgb_model = xgb_clf.fit(train_X, train_y)\n# xgb_preds = xgb_model.predict(val_X)\n# xgb_proba = xgb_model.predict_proba(val_X)\n\n# xgb_kappa_score = cohen_kappa_score(val_y, xgb_preds, weights='quadratic')\n\n# print(f'\\n****')\n# print(f'Accuracy of predictions is: {accuracy_score(val_y, xgb_preds)}')\n# # NB! Add weights='quadratic' to get same result as QWK \n# print(f'Skikit-learn Cohen Kappa Score (Quadratic) of predictions is: {cohen_kappa_score(val_y, xgb_preds, weights=\"quadratic\")}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model w Catboost"},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_train_model.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_train_model.filter(items=['installation_id', 'num_correct', 'num_incorrect', 'forecasted_assessment'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(X_train_model.columns)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Catboost Classification\n# # Important: X_scaled added 200121\n# from sklearn.model_selection import train_test_split\n# train_X, val_X, train_y, val_y = train_test_split(X_scaled, y, random_state = 0)\n\n# from catboost import CatBoostClassifier\n# from sklearn.metrics import accuracy_score\n\n# params_cb = {\n#             'max_depth' : 5,\n#             'learning_rate' : 0.01,\n#             'n_estimators' : 1493,\n#             'verbose' : 200,\n# #            'od_type': 'Iter',\n#             'loss_function' : 'MultiClass' #200109 new\n#             }\n\n# cbc_model = CatBoostClassifier(**params_cb)\n# cbc_model.fit(train_X, train_y)\n# #cbc_model.fit(train_X, train_y, eval_set=(val_X, val_y), early_stopping_rounds=10, use_best_model=True) #200119 use_best suggestion for bestIteration = 2679, Shrink model to first 2680 iterations\n# cbc_preds = cbc_model.predict(val_X)\n\n# # Save Catboost accuracy\n# cbc_score = accuracy_score(val_y, cbc_preds)\n# print(f'\\n****')\n# print(f'Accuracy of predictions is: {accuracy_score(val_y, cbc_preds)}')\n\n# # Check Cohen Kappa Score:\n# from sklearn.metrics import cohen_kappa_score\n# cbc_kappa_score = cohen_kappa_score(val_y, cbc_preds, weights='quadratic')\n\n# # NB! Add weights='quadratic' to get same result as QWK \n# print(f'Skikit-learn Cohen Kappa Score (Quadratic) of predictions is: {cohen_kappa_score(val_y, cbc_preds, weights=\"quadratic\")}')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model w Catboost regressor"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # CatBoostRegressor\n# # Stopped by overfitting detector  (10 iterations wait)\n# # bestTest = 0.8568023128\n# # bestIteration = 1141\n# # Shrink model to first 1142 iterations.\n# # ****\n# # Accuracy of predictions is: 0.734110203229486\n\n\n# from sklearn.model_selection import train_test_split\n# train_X, val_X, train_y, val_y = train_test_split(X_scaled, y, random_state = 0)\n\n# from catboost import CatBoostRegressor\n# #from sklearn.metrics import accuracy_score\n# from sklearn.metrics import mean_squared_error\n\n# params_cb = {\n#             'max_depth' : 5,\n#             'learning_rate' : 0.01,\n#             'n_estimators' : 1142,\n#             'verbose' : 200,\n# #            'od_type': 'Iter',\n#             'loss_function' : 'RMSE' #200109 new\n#             }\n\n# cbc_model = CatBoostRegressor(**params_cb)\n# cbc_model.fit(train_X, train_y)\n# #cbc_model.fit(train_X, train_y, eval_set=(val_X, val_y), early_stopping_rounds=10, use_best_model=True) #200119 use_best suggestion for bestIteration = 2679, Shrink model to first 2680 iterations\n# cbc_preds = cbc_model.predict(val_X)\n\n# # Save Catboost accuracy\n# cbc_score = mean_squared_error(val_y, cbc_preds)\n# print(f'\\n****')\n# print(f'Accuracy of predictions is: {mean_squared_error(val_y, cbc_preds)}')\n\n# # # Check Cohen Kappa Score:\n# # from sklearn.metrics import cohen_kappa_score\n# # cbc_kappa_score = cohen_kappa_score(val_y, cbc_preds, weights='quadratic')\n\n# # # NB! Add weights='quadratic' to get same result as QWK \n# # print(f'Skikit-learn Cohen Kappa Score (Quadratic) of predictions is: {cohen_kappa_score(val_y, cbc_preds, weights=\"quadratic\")}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # GridSearchCV\n# from sklearn.metrics import cohen_kappa_score, make_scorer\n# from sklearn.model_selection import GridSearchCV\n\n# kappa_scorer = make_scorer(cohen_kappa_score()\n# grid = GridSearchCV(CatBoostClassifier(), param_grid={'C': [1, 10]}, scoring=kappa_scorer)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # CV to assess model's quality\n# # Ref: https://scikit-learn.org/stable/modules/model_evaluation.html\n\n# # Creat \n# from sklearn.metrics import cohen_kappa_score, make_scorer\n# kappa_scorer = make_scorer(cohen_kappa_score)\n\n# # from sklearn import svm, datasets\n# from sklearn.model_selection import cross_val_score\n# clf_cbc = CatBoostClassifier(**params_cb)\n# #cross_val_score(clf_cbc, X, y, cv=5, scoring='accuracy')\n# #scores = cross_val_score(clf_cbc, X, y, cv=5, scoring='accuracy')\n# cross_val_score(clf_cbc, X, y, cv=5, scoring=kappa_scorer) # scoring=cohen_kappa_score\n\n# #cross_val_score(clf, X, y, cv=5, scoring='recall_macro')\n# #array([0.96..., 0.96..., 0.96..., 0.93..., 1.        ])\n# #>>> model = svm.SVC()\n# #>>> cross_val_score(model, X, y, cv=5, scoring='wrong_choice')\n# #Traceback (most recent call last):","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Permutation Importance\n\n# import eli5\n# from eli5.sklearn import PermutationImportance\n\n# perm = PermutationImportance(cbc_model, random_state=1).fit(val_X, val_y)\n# eli5.show_weights(perm, top=160, feature_names = list(feature_names)) #val_X.columns.tolist() -> list(feature_names)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Permutation Importance XGBoost\n\n# import eli5\n# from eli5.sklearn import PermutationImportance\n\n# perm = PermutationImportance(xgb_model, random_state=1).fit(val_X, val_y)\n# eli5.show_weights(perm, top=150, feature_names = val_X.columns.tolist())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"#type(val_X.forecasted_assessment[1087])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# PCA"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply PCA for dimension reduction\nfrom sklearn.decomposition import PCA\npca = PCA(n_components=10).fit(X)\nX_pca = pca.transform(X)\nprint(sum(pca.explained_variance_ratio_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model w LightGBM\n* First - Classifier"},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# len(X), len(y), len(train_X), len(train_y), len(val_X), len(val_y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true,"_kg_hide-output":true},"cell_type":"code","source":"# # Light GBM Classifier\n\n# from sklearn.model_selection import train_test_split\n# train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.2765, random_state = 0)\n\n# import lightgbm as lgb\n# # from sklearn.metrics import accuracy_score\n# from sklearn.metrics import log_loss\n# # create dataset for lightgbm\n# lgb_train = lgb.Dataset(train_X, train_y)\n# lgb_eval = lgb.Dataset(val_X, val_y)\n\n# # specify parameters\n# params_lgb = {\n#             'boosting_type': 'gbdt',\n#             'objective': 'multiclass', #            'objective': 'multiclass',\n#             'num_class': 4,\n#             'metric': '',\n#             'num_leaves': 31,\n#             'learning_rate': 0.01,\n#             'feature_fraction': 0.9,\n#             'bagging_fraction': 0.8,\n#             'bagging_freq': 5,\n#             'verbose': 0,\n#            'is_unbalance': True,\n#             'num_iterations': 3000\n#             }\n\n# print('Starting training...')\n# # train\n# gbm_model = lgb.train(params_lgb,\n#                       lgb_train,\n#                      num_boost_round=20,\n#                      valid_sets=lgb_eval,\n#                      early_stopping_rounds=5\n#                      )\n\n# print('Starting predicting...')\n# # predict\n# gbm_pred = gbm_model.predict(val_X, num_iteration=gbm_model.best_iteration)\n# # eval\n# print(':', )\n# print(f'log_loss of predictions is: {log_loss(val_y, gbm_preds)}')\n# #print(f'Accuracy of predictions is: {accuracy_score(val_y, gbm_preds)}')\n# #print(f'Skikit-learn Cohen Kappa Score (Quadratic) of predictions is: {cohen_kappa_score(val_y, gbm_pred, weights=\"quadratic\")}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LGBM with PCA X_pca\n\nfrom sklearn.model_selection import train_test_split\ntrain_X, val_X, train_y, val_y = train_test_split(X_pca, y, random_state = 0)\n\nimport lightgbm as lgb\n# from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import mean_squared_error\n\n# create dataset for lightgbm\nlgb_train = lgb.Dataset(train_X, label=train_y)\nlgb_eval = lgb.Dataset(val_X, label=val_y, reference=lgb_train)\n\n# specify parameters\nparams_lgb = {'n_estimators': 10000,\n            'boosting_type': 'gbdt',\n            'objective': 'regression',\n            'metric': 'rmse',\n            'subsample': 0.75,\n            'subsample_freq': 1,\n            'learning_rate': 0.04,\n            'feature_fraction': 0.9,\n             'max_depth': 15,\n            'lambda_l1': 1,  \n            'lambda_l2': 1,\n            'verbose': 100,\n#            'early_stopping_rounds': 100, \n            'eval_metric': 'cappa'\n            }\n\nprint('Starting training...')\n# train\ngbm_model = lgb.train(params_lgb, lgb_train, num_boost_round=20, valid_sets=lgb_eval) #, early_stopping_rounds=100)\n#gbm_model = lgb.train(params_lgb, lgb_train, valid_sets=lgb_eval) #new200120 , verbose_eval=verbosity\n\nprint('Starting predicting...')\n# predict\ngbm_pred = gbm_model.predict(val_X, num_iteration=gbm_model.best_iteration)\n# eval\nprint(':', )\nprint(f'The rmse of prediction is: {mean_squared_error(val_y, gbm_pred) ** 0.5}')\n#print(f'Skikit-learn Cohen Kappa Score (Quadratic) of predictions is: {cohen_kappa_score(val_y, gbm_pred, weights=\"quadratic\")}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_kg_hide-input":true},"cell_type":"code","source":"# # Ref: https://github.com/microsoft/LightGBM/blob/master/examples/python-guide/simple_example.py\n\n# from sklearn.model_selection import train_test_split\n# train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)\n\n# import lightgbm as lgb\n# # from sklearn.metrics import accuracy_score\n# from sklearn.metrics import mean_squared_error\n\n# # create dataset for lightgbm\n# lgb_train = lgb.Dataset(train_X, label=train_y)\n# lgb_eval = lgb.Dataset(val_X, label=val_y, reference=lgb_train)\n\n# # specify parameters\n# params_lgb = {'n_estimators': 10000,\n#             'boosting_type': 'gbdt',\n#             'objective': 'regression',\n#             'metric': 'rmse',\n#             'subsample': 0.75,\n#             'subsample_freq': 1,\n#             'learning_rate': 0.04,\n#             'feature_fraction': 0.9,\n#              'max_depth': 15,\n#             'lambda_l1': 1,  \n#             'lambda_l2': 1,\n#             'verbose': 100,\n#             'early_stopping_rounds': 100, 'eval_metric': 'cappa'\n#             }\n\n# print('Starting training...')\n# # train\n# #gbm_model = lgb.train(params_lgb, lgb_train, num_boost_round=20, valid_sets=lgb_eval, early_stopping_rounds=5)\n# gbm_model = lgb.train(params_lgb, lgb_train, valid_sets=lgb_eval) #new200120 , verbose_eval=verbosity\n\n# print('Starting predicting...')\n# # predict\n# gbm_pred = gbm_model.predict(val_X, num_iteration=gbm_model.best_iteration)\n# # eval\n# print(':', )\n# print(f'The rmse of prediction is: {mean_squared_error(val_y, gbm_pred) ** 0.5}')\n# #print(f'Skikit-learn Cohen Kappa Score (Quadratic) of predictions is: {cohen_kappa_score(val_y, gbm_pred, weights=\"quadratic\")}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Train on all dataset LightGBM Reg\n\n# import lightgbm as lgb\n# from sklearn.metrics import mean_squared_error\n\n# # Create dataset for lightgbm on full train set\n# lgb_train = lgb.Dataset(X, label=y)\n\n# # specify parameters\n# params_lgb = {'n_estimators': 142,\n#             'boosting_type': 'gbdt',\n#             'objective': 'regression',\n#             'metric': 'rmse',\n#             'subsample': 0.75,\n#             'subsample_freq': 1,\n#             'learning_rate': 0.04,\n#             'feature_fraction': 0.9,\n#              'max_depth': 15,\n#             'lambda_l1': 1,  \n#             'lambda_l2': 1,\n#             'verbose': 100,\n# #            'early_stopping_rounds': 100, \n#             'eval_metric': 'cappa'\n#             }\n\n# print('Starting training...')\n# # train\n# #gbm_model = lgb.train(params_lgb, lgb_train, num_boost_round=20, valid_sets=lgb_eval, early_stopping_rounds=5)\n# gbm_model = lgb.train(params_lgb, lgb_train) #new200120 , verbose_eval=verbosity\n# print('Training done...')\n\n# print('***')\n# print('Starting predicting...')\n# # predict\n# gbm_pred = gbm_model.predict(X, num_iteration=gbm_model.best_iteration)\n# # eval\n# print('***')\n# print(f'The rmse of prediction is: {mean_squared_error(y, gbm_pred) ** 0.5}')\n# #print(f'Skikit-learn Cohen Kappa Score (Quadratic) of predictions is: {cohen_kappa_score(y, gbm_pred, weights=\"quadratic\")}')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # LightGBM\n\n# submission = pd.read_csv(path + 'sample_submission.csv')\n# gbm_preds = gbm_model.predict(X_test_gt)\n\n# submission['accuracy_group'] = gbm_preds\n\n# submission['accuracy_group_weight0'] = np.where((submission['accuracy_group'] <= 0.81387600), 0, 0)\n# submission['accuracy_group_weight1'] = np.where((submission['accuracy_group'] > 0.81387600) & (submission['accuracy_group'] <= 1.09392700), 1, 0)\n# submission['accuracy_group_weight2'] = np.where((submission['accuracy_group'] > 1.09392700) & (submission['accuracy_group'] <= 1.42779600), 2, 0)\n# submission['accuracy_group_weight3'] = np.where((submission['accuracy_group'] > 1.42779600), 3, 0)\n# submission['accuracy_group'] = submission['accuracy_group_weight0'] + submission['accuracy_group_weight1'] + submission['accuracy_group_weight2'] + submission['accuracy_group_weight3']\n# submission = submission.drop(['accuracy_group_weight0', 'accuracy_group_weight1', 'accuracy_group_weight2', 'accuracy_group_weight3'], axis=1)\n\n# submission.to_csv(\"submission.csv\", index = False)\n\n# submission.accuracy_group.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# gbm_preds = gbm_model.predict(X_test_gt)\n# submission['accuracy_group'] = np.round(gbm_preds).astype(int)\n# submission.to_csv(\"submission.csv\", index = False)\n# submission.head()\n# submission.accuracy_group.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del X_train_model\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Preparing test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Preparing test set\nX_test = pd.read_csv(path + 'test.csv', usecols = load_columns)\nsubmission = pd.read_csv(path + 'sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def extract_accuracy_set_test(df):\n    X_test_gt = pd.DataFrame(data=None)\n    \n    # X_test_gt will be used only for accuracy features extraction\n    # First, filter assessment events only\n    # Second, drop columns which will be processed separately\n    \n    X_test_gt = df[((df['event_code'] == 4100) & \n                     (df['title'].isin(['Cart Balancer (Assessment)', \n                                        'Cauldron Filler (Assessment)', \n                                        'Mushroom Sorter (Assessment)', \n                                        'Chest Sorter (Assessment)']))) | \n                    ((df['event_code'] == 4110) & \n                     (df['title'] == 'Bird Measurer (Assessment)'))].copy(deep=True)\n\n    \n#     #quick add of assessments_time\n    \n#     X_test_game_time = X_test_gt.groupby(['installation_id', 'game_session'], as_index=False, sort=False)['game_time'].last()\n#     X_test_game_time = X_test_game_time.groupby('installation_id', as_index=False, sort=False)['game_time'].sum()\n    \n    X_test_gt.drop(['event_id', \n                     'timestamp', \n                     'event_count', \n                     'event_code', \n                     'game_time',\n                     'type',\n                     'world',], axis=1, inplace=True)\n    \n    # Third, extract correct and incorrect assessment attempts per user from 'event_data'\n    # Create num_correct and num_incorrect columns\n    \n    corr = '\"correct\":true'\n    incorr = '\"correct\":false'\n    \n    X_test_gt['num_correct'] = X_test_gt['event_data'].apply(lambda x: 1 if corr in x else 0)\n    X_test_gt['num_incorrect'] = X_test_gt['event_data'].apply(lambda x: 1 if incorr in x else 0)\n    \n    # Fourth, aggregate (sum) correct and incorrect assessment attempts \n    # per 'game_session', 'installation_id' and assessment 'title'\n    # As provided in grount truth (labels.csv)\n    \n    X_test_gt = X_test_gt.sort_values(['installation_id', 'game_session'], ascending=True).groupby(['game_session', 'installation_id', 'title'], as_index=False, sort=False).agg(sum)\n    \n    # Fifths, create 'accuracy' feature = corr / (corre + incorr)\n    \n    X_test_gt['accuracy'] = X_test_gt['num_correct'] / (X_test_gt['num_correct'] + X_test_gt['num_incorrect'])\n    \n    # Sixths, create 'accuracy_group' feature\n    # 3: the assessment was solved on the first attempt\n    # 2: the assessment was solved on the second attempt\n    # 1: the assessment was solved after 3 or more attempts\n    # 0: the assessment was never solved\n\n    # If accuracy is 0.0 (no correct attempts), accuracy group is 0 as all observations in X_test_gt by now has at least one attempt\n    # If accuracy is 1.0 (that is no incorrect attempts), accuracy group is 3\n    # If accuracy is 0.5 (there is equal amount of correct and incorrect attempts), accuracy group is 2\n    # Any other case means that accuracy group equals 1, that is 3 or more attempts were needed to make a correct attempt    \n\n    X_test_gt['accuracy_group'] = X_test_gt['accuracy'].apply(lambda x: 0 if x == 0.0 else (3 if x == 1.0 else (2 if x == 0.5 else 1)))\n   \n    return X_test_gt\n\nX_test_gt = extract_accuracy_set_test(X_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# debugging\ndebugging_ids(X_test_gt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### (T) Assessment count\n**Adjusted** for test set as:\n* not all users took assessment\n* in test.csv our forecasted assessment is not under 4100 or 4110 code, therefore does not include in gt df\n* feature shows how many unique assessments user took before, not total count of non-unique assessments"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating the last assessment coll\nX_test_gt['previous_assessments_count'] = X_test_gt.groupby('installation_id')['title'].transform('count')\n# Difference with train prep:\n# No need to reduce by one as last one under 4100 or 4110 code is not the one we are forecasting\n# X_test_gt['previous_assessments_count'] = X_test_gt['previous_assessments_count'].apply(lambda x: x -1 if x > 1 else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#X_test_gt.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_test[(X_test['installation_id'] == '01242218') & ((X_test['event_code'] == 4100) | (X_test['event_code'] == 4110))]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# debugging\ndebugging_ids(X_test_gt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### (~T) Accuracy groups\n\n* Should be fine as we do not have forecasted assessment's, that is do not count additional 0 accuracy_group"},{"metadata":{"trusted":true},"cell_type":"code","source":"#Accuracy groups\nX_test_gt['acc_0'] = X_test_gt['accuracy_group'].apply(lambda x: 1 if x == 0 else 0)\nX_test_gt['acc_1'] = X_test_gt['accuracy_group'].apply(lambda x: 1 if x == 1 else 0)\nX_test_gt['acc_2'] = X_test_gt['accuracy_group'].apply(lambda x: 1 if x == 2 else 0)\nX_test_gt['acc_3'] = X_test_gt['accuracy_group'].apply(lambda x: 1 if x == 3 else 0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# X_test_gt.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# debugging\ndebugging_ids(X_test_gt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### (T) accuracy_group per assessment title"},{"metadata":{"trusted":true},"cell_type":"code","source":"# 'accuracy_group' per assessment 'title'\n# Ref: https://stackoverflow.com/questions/27474921/compare-two-columns-using-pandas/27475029\n# (condition, output value, else)\n\nX_test_gt['bird_accg_0'] = np.where((X_test_gt['title'] == 'Bird Measurer (Assessment)') & (X_test_gt['accuracy_group'] == 0), 1, 0)\nX_test_gt['bird_accg_1'] = np.where((X_test_gt['title'] == 'Bird Measurer (Assessment)') & (X_test_gt['accuracy_group'] == 1), 1, 0)\nX_test_gt['bird_accg_2'] = np.where((X_test_gt['title'] == 'Bird Measurer (Assessment)') & (X_test_gt['accuracy_group'] == 2), 1, 0)\nX_test_gt['bird_accg_3'] = np.where((X_test_gt['title'] == 'Bird Measurer (Assessment)') & (X_test_gt['accuracy_group'] == 3), 1, 0)\n\nX_test_gt['cart_accg_0'] = np.where((X_test_gt['title'] == 'Cart Balancer (Assessment)') & (X_test_gt['accuracy_group'] == 0), 1, 0)\nX_test_gt['cart_accg_1'] = np.where((X_test_gt['title'] == 'Cart Balancer (Assessment)') & (X_test_gt['accuracy_group'] == 1), 1, 0)\nX_test_gt['cart_accg_2'] = np.where((X_test_gt['title'] == 'Cart Balancer (Assessment)') & (X_test_gt['accuracy_group'] == 2), 1, 0)\nX_test_gt['cart_accg_3'] = np.where((X_test_gt['title'] == 'Cart Balancer (Assessment)') & (X_test_gt['accuracy_group'] == 3), 1, 0)\n\nX_test_gt['cauldron_accg_0'] = np.where((X_test_gt['title'] == 'Cauldron Filler (Assessment)') & (X_test_gt['accuracy_group'] == 0), 1, 0)\nX_test_gt['cauldron_accg_1'] = np.where((X_test_gt['title'] == 'Cauldron Filler (Assessment)') & (X_test_gt['accuracy_group'] == 1), 1, 0)\nX_test_gt['cauldron_accg_2'] = np.where((X_test_gt['title'] == 'Cauldron Filler (Assessment)') & (X_test_gt['accuracy_group'] == 2), 1, 0)\nX_test_gt['cauldron_accg_3'] = np.where((X_test_gt['title'] == 'Cauldron Filler (Assessment)') & (X_test_gt['accuracy_group'] == 3), 1, 0)\n\nX_test_gt['chest_accg_0'] = np.where((X_test_gt['title'] == 'Chest Sorter (Assessment)') & (X_test_gt['accuracy_group'] == 0), 1, 0)\nX_test_gt['chest_accg_1'] = np.where((X_test_gt['title'] == 'Chest Sorter (Assessment)') & (X_test_gt['accuracy_group'] == 1), 1, 0)\nX_test_gt['chest_accg_2'] = np.where((X_test_gt['title'] == 'Chest Sorter (Assessment)') & (X_test_gt['accuracy_group'] == 2), 1, 0)\nX_test_gt['chest_accg_3'] = np.where((X_test_gt['title'] == 'Chest Sorter (Assessment)') & (X_test_gt['accuracy_group'] == 3), 1, 0)\n\nX_test_gt['mushroom_accg_0'] = np.where((X_test_gt['title'] == 'Mushroom Sorter (Assessment)') & (X_test_gt['accuracy_group'] == 0), 1, 0)\nX_test_gt['mushroom_accg_1'] = np.where((X_test_gt['title'] == 'Mushroom Sorter (Assessment)') & (X_test_gt['accuracy_group'] == 1), 1, 0)\nX_test_gt['mushroom_accg_2'] = np.where((X_test_gt['title'] == 'Mushroom Sorter (Assessment)') & (X_test_gt['accuracy_group'] == 2), 1, 0)\nX_test_gt['mushroom_accg_3'] = np.where((X_test_gt['title'] == 'Mushroom Sorter (Assessment)') & (X_test_gt['accuracy_group'] == 3), 1, 0)\n\n# debugging\ndebugging_ids(X_test_gt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### (T) Accuracy (num_correct, num_incorrect, accuracy) per assessment"},{"metadata":{"trusted":true},"cell_type":"code","source":"# {title}_correct, {title}_incorrect, {title}_accuracy per 'installation_id' per assessment 'title'\n# Ref: https://stackoverflow.com/questions/27474921/compare-two-columns-using-pandas/27475029\n# (condition, output value, else)\n# E.g. if Bird Measurer has num_correct = 1, add 1, elsewise add 0\n# If Bird Measurer has num_incorrect = 12, add 12, elsewise add 0\n\nX_test_gt['bird_correct'] = np.where((X_test_gt['title'] == 'Bird Measurer (Assessment)') & (X_test_gt['num_correct'] == 1), 1, 0)\nX_test_gt['bird_incorrect'] = np.where((X_test_gt['title'] == 'Bird Measurer (Assessment)') & (X_test_gt['num_incorrect'] > 0), X_test_gt['num_incorrect'], 0)\nX_test_gt['bird_accuracy'] = np.where((X_test_gt['title'] == 'Bird Measurer (Assessment)'), X_test_gt['accuracy'], 0)\n\nX_test_gt['cart_correct'] = np.where((X_test_gt['title'] == 'Cart Balancer (Assessment)') & (X_test_gt['num_correct'] == 1), 1, 0)\nX_test_gt['cart_incorrect'] = np.where((X_test_gt['title'] == 'Cart Balancer (Assessment)') & (X_test_gt['num_incorrect'] > 0), X_test_gt['num_incorrect'], 0)\nX_test_gt['cart_accuracy'] = np.where((X_test_gt['title'] == 'Cart Balancer (Assessment)'), X_test_gt['accuracy'], 0)\n\nX_test_gt['cauldron_correct'] = np.where((X_test_gt['title'] == 'Cauldron Filler (Assessment)') & (X_test_gt['num_correct'] == 1), 1, 0)\nX_test_gt['cauldron_incorrect'] = np.where((X_test_gt['title'] == 'Cauldron Filler (Assessment)') & (X_test_gt['num_incorrect'] > 0), X_test_gt['num_incorrect'], 0)\nX_test_gt['cauldron_accuracy'] = np.where((X_test_gt['title'] == 'Cauldron Filler (Assessment)'), X_test_gt['accuracy'], 0)\n\nX_test_gt['chest_correct'] = np.where((X_test_gt['title'] == 'Chest Sorter (Assessment)') & (X_test_gt['num_correct'] == 1), 1, 0)\nX_test_gt['chest_incorrect'] = np.where((X_test_gt['title'] == 'Chest Sorter (Assessment)') & (X_test_gt['num_incorrect'] > 0), X_test_gt['num_incorrect'], 0)\nX_test_gt['chest_accuracy'] = np.where((X_test_gt['title'] == 'Chest Sorter (Assessment)'), X_test_gt['accuracy'], 0)\n\nX_test_gt['mushroom_correct'] = np.where((X_test_gt['title'] == 'Mushroom Sorter (Assessment)') & (X_test_gt['num_correct'] == 1), 1, 0)\nX_test_gt['mushroom_incorrect'] = np.where((X_test_gt['title'] == 'Mushroom Sorter (Assessment)') & (X_test_gt['num_incorrect'] > 0), X_test_gt['num_incorrect'], 0)\nX_test_gt['mushroom_accuracy'] = np.where((X_test_gt['title'] == 'Mushroom Sorter (Assessment)'), X_test_gt['accuracy'], 0)\n\n# debugging\ndebugging_ids(X_test_gt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### (T) Aggregation of features\n\n* Leaving single row per 'installation_id'\n\n##### Headline in train: Saving the index of last (forecasted) assessment\n\n* No need to separate FC assessments row from the rest as it is not included in test set\n* Will perform only aggregation"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Not applicable to test set:\n# # We prepare a dataframe which stores the index of last assessment of each installation_id with assessment attempt\n# last_observations_index_df = X_test_gt.reset_index().groupby('installation_id', as_index=False)['index'].last()\n# last_observations_index_list = list(last_observations_index_df['index']) \n# X_test_gt.drop(['game_session', 'title'], axis=1, inplace=True)\n# # Creating a copy dataframe with last_observations and without them\n# X_test_gt_last = X_test_gt.loc[X_test_gt.index.isin(last_observations_index_list)]\n# X_test_gt_remainder = X_test_gt.loc[~X_test_gt.index.isin(last_observations_index_list)]\n\nX_test_gt_remainder_sum_list = X_train_gt_sum_list\n\n# X_test_gt_remainder_sum_list = ['num_correct', 'num_incorrect', \n#        'bird_correct', 'bird_incorrect',\n#        'cart_correct', 'cart_incorrect', 'cauldron_correct',\n#        'cauldron_incorrect', 'chest_correct',\n#        'chest_incorrect', 'mushroom_correct',\n#        'mushroom_incorrect', 'acc_0',\n#        'acc_1', 'acc_2', 'acc_3', 'bird_accg_0', 'bird_accg_1', 'bird_accg_2',\n#        'bird_accg_3', 'cart_accg_0', 'cart_accg_1', 'cart_accg_2',\n#        'cart_accg_3', 'cauldron_accg_0', 'cauldron_accg_1', 'cauldron_accg_2',\n#        'cauldron_accg_3', 'chest_accg_0', 'chest_accg_1', 'chest_accg_2',\n#        'chest_accg_3', 'mushroom_accg_0', 'mushroom_accg_1', 'mushroom_accg_2',\n#        'mushroom_accg_3']\n\nX_test_gt_remainder_mean_list = X_train_gt_mean_list\n\n# X_test_gt_remainder_mean_list = ['accuracy',\n#        'accuracy_group', 'bird_accuracy',\n#        'cart_accuracy', 'cauldron_accuracy', 'chest_accuracy', 'mushroom_accuracy']\n\n# !!! Should add 'forecasted_assessment'\n# Removed 'sessions_with_assessment_count'\nX_test_gt_remainder_unchanged_list = ['previous_assessments_count']\n\n# Difference in train set:\n# X_test_gt_remainder_unchanged_list = ['Y_target', 'forecasted_assessment', 'previous_assessments_count', 'sessions_with_assessment_count'] \n\n# Difference in train set:\n# We do not define X_test_gt_remainder and take all in X_test_gt\nX_test_gt_sum = X_test_gt.groupby(['installation_id'], as_index=False, sort=False)[X_test_gt_remainder_sum_list].agg(sum)\nX_test_gt_mean = X_test_gt.groupby(['installation_id'], as_index=False, sort=False)[X_test_gt_remainder_mean_list].agg('mean')\nX_test_gt_unchaged = X_test_gt.groupby(['installation_id'], as_index=False, sort=False)[X_test_gt_remainder_unchanged_list].last()\n\n# Merge both\nX_test_gt_remainder = pd.merge(X_test_gt_sum, X_test_gt_mean, how='left', on=['installation_id'])\nX_test_gt = pd.merge(X_test_gt_remainder, X_test_gt_unchaged, how='left', on=['installation_id'])\n\n# Not applicable to test set:\n# # Returning the installation_ids which had no previous assessments before the forecasted one\n# #X_test_gt = pd.concat([X_test_gt_remainder, X_test_gt_last]).sort_index().reset_index(drop=True) index got broken while grouping by\n# X_test_gt = X_test_gt_remainder.append(X_test_gt_last, ignore_index=True)\n\n# # Questionable re sorting as it drops installation_id, need to test\n# X_test_gt = pd.concat([X_test_gt_remainder, X_test_gt_last]).drop_duplicates('installation_id').reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_gt.head(5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# debugging\ndebugging_ids(X_test_gt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # !debugging, finding heavy user\n# X_test_gt[X_test_gt['num_correct'] == X_test_gt.num_correct.max()]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # !debugging on heavy user\n# X_test[(X_test['installation_id'] == '56a739ec') & (X_test['event_code'] == 4100) & (X_test['title'] == 'Cart Balancer (Assessment)')]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Adding users w/o previous assessment attempts\n\n* Test set specific as in train set we used only 'intallation_id's with at least one assessment attempt "},{"metadata":{"trusted":true},"cell_type":"code","source":"test_features_list = X_test_gt.columns\nX_test_gt.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_users_wo_assessments = set(X_test.installation_id) - set(X_test_gt.installation_id)\nlen(test_users_wo_assessments)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Creating empty df matching test's columns\n\n* Filled with 0\n* Alternatively could test with Nan, None or -1"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_users_wo_assessments_df = pd.DataFrame(0, index=np.arange(len(test_users_wo_assessments)), columns=test_features_list)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_users_wo_assessments_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Adding 'installation_id's w/o prior assessments"},{"metadata":{"trusted":true},"cell_type":"code","source":"# We have created installation_id column with zero values. Now will assign missing installation_id\ntest_users_wo_assessments_df['installation_id'] = test_users_wo_assessments","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_users_wo_assessments_df.head(2)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### (~T) Merging 'installation_id's with and w/o assessments"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_gt = X_test_gt.append(test_users_wo_assessments_df, ignore_index=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# debugging\ndebugging_ids(X_test_gt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# debugging\nlen(set(X_test_gt.installation_id))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# debugging\n# we lost the order of 'installation_id', but submission is sorted ascending\nbooltest_sub = X_test_gt.installation_id.sort_values(ascending=True).reset_index(drop=True) == submission.installation_id\nset(booltest_sub)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### (T) Sorting to match order of submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_gt = X_test_gt.sort_values('installation_id', ascending=True).reset_index(drop=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_gt.head(10)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# debugging sorting\nbooltest_train = X_test_gt.installation_id == submission.installation_id\nset(booltest_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# debugging\ndebugging_ids(X_test_gt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### (T) Adding 'forecasted_assessment' feature\n\n* To both 'installation_id's with and w/o assessment attempt\n* It fixes initial bug where 'installation_id's w/o assessment attempt got their last attempted assessment as their 'forecasted_assessment' "},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create the forecasted_assessment_df which will contain all test set's installation_ids last forecasted_assessment\n\nforecasted_assessment_df = X_test.groupby(['installation_id'], as_index=False, sort=False).agg('last')\n\n# Reduce forecasted_assessment_df to users only w/o assessment (1000 -> 443):\n# forecasted_assessment_df = forecasted_assessment_df[forecasted_assessment_df.installation_id.isin(test_users_wo_assessments)]\n# Reseting the index, otherwise will get Nans when mapping:\n# forecasted_assessment_df.reset_index()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# forecasted_assessment_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Add 'forecasted_assessment' feature to the test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Add forecasted_assessment number to X_test_gt:\n# Map is how train set has assigned values to assessment titles:\n# 0 Bird Measurer (Assessment)\n# 1 Cart Balancer (Assessment)\n# 2 Cauldron Filler (Assessment)\n# 3 Chest Sorter (Assessment)\n# 4 Mushroom Sorter (Assessment)\nX_test_gt['forecasted_assessment'] = forecasted_assessment_df['title'].map({'Bird Measurer (Assessment)': 0,\n                                                                                               'Cart Balancer (Assessment)': 1, \n                                                                                               'Cauldron Filler (Assessment)': 2, \n                                                                                               'Chest Sorter (Assessment)': 3, \n                                                                                    'Mushroom Sorter (Assessment)': 4})","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_gt.head(2)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# debugging\nset(X_test_gt.forecasted_assessment), X_test_gt.forecasted_assessment.count()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# debugging\ndebugging_ids(X_test_gt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # debugging\n# X_test_gt.loc[441, ['forecasted_assessment']]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # debugging\n# X_test_gt.loc[441,]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # debugging OK - 'forecasted_assessment' of '779b71a3' is 'Chest Sorter (Assessment)' or encoded 3 \n# X_test[X_test['installation_id'] == '779b71a3'].tail()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Adding none acc features to the test set"},{"metadata":{},"cell_type":"markdown","source":"## (~T) event_code\n\n#### Preparing event_code features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Uses ~3 GB of RAM for this operation (9->12->9)\nX_test_eventcode = X_test.filter(['installation_id', 'event_code'], axis=1)\nX_test_eventcode = event_code(X_test)\n\n#### Merging event_code features to the main test set\n\n# Add event_code features to the main dataframe\nX_test_gt = pd.merge(X_test_gt, X_test_eventcode, on=['installation_id'])\n\ndel X_test_eventcode\ngc.collect()\n\n# debugging\ndebugging_ids(X_test_gt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## (~T) Title, type, world and event_code\n\n#### Preparing title, type and world features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create new X_test_titletypeworldfeat which holds time only title, type and world features\nX_test_titletypeworldfeat = X_test.filter(['installation_id', 'title', 'type', 'world'], axis=1)\nX_test_titletypeworldfeat = title_type_world(X_test_titletypeworldfeat)\n\n#### Merging title, type and world features to the main test set\n\n# Add title, type and world features to the main dataframe\nX_test_gt = pd.merge(X_test_gt, X_test_titletypeworldfeat, on=['installation_id'])\n# # Count nan in df for debugging purposes\n# X_test_gt.isna().sum()\n\ndel X_test_titletypeworldfeat\ngc.collect()\n\n# debugging\ndebugging_ids(X_test_gt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## (~T) Other features\n\n* all_actions_time\n\n* Aggregate amount (in ms) of time spent on Assessments, Activities and Games\n* Clips do not have time spent feature\n\n* all_actions_time\n* action_duration_mean (!!!)\n* event_code_count_mean\n* number_of_sessions_nu\n* event_count_mean (!!!)"},{"metadata":{},"cell_type":"markdown","source":"###### all_actions_time"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating all_actions_time (games, activities and assessments)\n# RAM: 8.7->8.5-8.7 GB\nfeat_gametime = X_test[X_test['type'].isin(['Assessment', 'Game', 'Activity'])]\nfeat_gametime = feat_gametime.groupby(['installation_id', 'game_session'], as_index=False, sort=False)['game_time'].last()\nfeat_gametime = feat_gametime.groupby('installation_id', as_index=False, sort=False)['game_time'].sum()\n\n# Merging to the main test set\nX_test_gt['all_actions_time'] = feat_gametime['game_time']\n\n# Deleting\ndel feat_gametime\ngc.collect()\n\n# debugging\ndebugging_ids(X_test_gt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### action_duration_mean"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating action_duration_mean (games, activities and assessments) (!!!)\n# RAM: 8.7->9.6->8.7 GB\nfeat_gametimemean = X_test[X_test['type'].isin(['Assessment', 'Game', 'Activity'])]\nfeat_gametimemean = feat_gametimemean.groupby(['installation_id', 'game_session'], as_index=False, sort=False)['game_time'].last()\nfeat_gametimemean = feat_gametimemean.groupby('installation_id', as_index=False, sort=False)['game_time'].mean()\n\n# Merging to the main test set\nX_test_gt['action_duration_mean'] = feat_gametimemean['game_time']\n\n# Deleting\ndel feat_gametimemean\ngc.collect()\n\n# debugging\ndebugging_ids(X_test_gt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###### event_code_count_mean"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating event_code_count_mean (!!!)\n# RAM: OK, flat\nfeat_eventcodecountmean = X_test.groupby(['installation_id', 'game_session'], as_index=False, sort=False)['event_code'].count()\nfeat_eventcodecountmean = feat_eventcodecountmean.groupby('installation_id', as_index=False, sort=False)['event_code'].mean()\n\n# Merging to the main test set\nX_test_gt['event_code_count_mean'] = feat_eventcodecountmean['event_code']\n\n# Deleting\ndel feat_eventcodecountmean\ngc.collect()\n\n# debugging\ndebugging_ids(X_test_gt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### number_of_sessions_nu"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating event_code_count_mean\n# RAM: OK, flat\nfeat_numberofsessions = X_test.groupby(['installation_id'], as_index=False, sort=False)['game_session'].count()\n\n# Merging to the main test set\nX_test_gt['number_of_sessions_nu'] = feat_numberofsessions['game_session']\n\n# Deleting\ndel feat_numberofsessions\ngc.collect()\n\n# debugging\ndebugging_ids(X_test_gt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### event_count_mean"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Creating event_count_mean (!!!)\n# RAM: OK, flat\nfeat_eventcountmean = X_test.groupby(['installation_id', 'game_session'], as_index=False, sort=False)['event_count'].last()\nfeat_eventcountmean = feat_eventcountmean.groupby('installation_id', as_index=False, sort=False)['event_count'].mean()\n\n# Merging to the main test set\nX_test_gt['event_count_mean'] = feat_eventcountmean['event_count']\n\n# Deleting\ndel feat_eventcountmean\ngc.collect()\n\n# debugging\ndebugging_ids(X_test_gt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### (~T) timestamp\n\n#### bug - taking the last even, which might be not assessment\n#### could replace with mean"},{"metadata":{"trusted":true},"cell_type":"code","source":"# RAM 8.7->10->9.3\n# Create new X_test_timefeat which holds time only features  \nfeat_time = X_test.filter(['installation_id', 'timestamp'], axis=1)\n# Prepare time features from given timestamp \nfeat_time = timestamp_split(feat_time)\n\n# Defining as last (bug)\nfeat_time = feat_time.groupby('installation_id', as_index=False).last()\n\n# Merging to the main test set\nX_test_gt = pd.merge(X_test_gt, feat_time, on=['installation_id'])\n\n# Deleting\ndel feat_time\ngc.collect()\n\n#del X_test\ngc.collect()\n\n# debugging\ndebugging_ids(X_test_gt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### (legacy) title, type and world"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Re-using f-ion used in train\n# # Create new titletypeworldfeat which holds time only title, type and world features  \n# X_test_titletypeworldfeat = X_test.filter(['installation_id', 'title', 'type', 'world'], axis=1)\n# # Prepare title, type and world features from given timestamp \n# X_test_titletypeworldfeat = title_type_world(X_test_titletypeworldfeat)\n# X_test_titletypeworldfeat","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # debugging OK, 'ffe00ca8' has 5 rows in 'world' 'CRYSTALCAVES'\n# X_test[(X_test['installation_id'] == 'ffe00ca8') & (X_test['world'] == 'CRYSTALCAVES')]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### (legacy) merge of timestamp, type, title and world features to main test set"},{"metadata":{},"cell_type":"markdown","source":"##### debugging index before merger\n\n* to avoid incorrectly assigning features from another 'installation_id's  "},{"metadata":{"trusted":true},"cell_type":"code","source":"# # debugging sorting of timefeat\n\n# booltest_timefeat = X_test_gt.installation_id == X_test_timefeat.installation_id\n# set(booltest_timefeat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # debugging sorting of X_test_titletypeworldfeat\n\n# booltest_titletypeworldfeat = X_test_gt.installation_id == X_test_titletypeworldfeat.installation_id\n# set(booltest_titletypeworldfeat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # debugging\n# debugging_ids(X_test_gt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### merging time features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # debugging\n# debugging_ids(X_test_timefeat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Merging new features to main test set\n\n# # Add time features to the main dataframe\n# X_test_gt = pd.merge(X_test_gt, X_test_timefeat, on=['installation_id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# len(set(X_test_gt.installation_id)), X_test_gt.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### merging titletypeworld features"},{"metadata":{"trusted":true},"cell_type":"code","source":"# # debugging - count nan in df - OK\n# X_test_gt.isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Add title, type and world features to the main dataframe\n# X_test_gt = pd.merge(X_test_gt, X_test_titletypeworldfeat, on=['installation_id'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# len(set(X_test_gt.installation_id)), X_test_gt.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Count nan in df for debugging purposes\n#set(X_test_gt.isna().sum())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # debugging sorting\n# booltest_sub = X_test_gt.installation_id == submission.installation_id\n# set(booltest_sub)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Cleaning unused dfs and variables"},{"metadata":{"trusted":true},"cell_type":"code","source":"#del X_test, X_test_gt_remainder_sum_list, X_test_gt_remainder_mean_list, X_test_gt_remainder_unchanged_list, X_test_gt_sum, X_test_gt_mean, X_test_gt_unchaged, test_features_list, test_users_wo_assessments, test_users_wo_assessments_df, forecasted_assessment_df, X_test_timefeat, X_test_titletypeworldfeat\ngc.collect()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## (~T) all_actions_time\n\n* Aggregate amount (in ms) of time spent on Assessments, Activities and Games\n* Clips do not have time spent feature"},{"metadata":{"trusted":true},"cell_type":"code","source":"# #### Adding feature all_actions_time \n# feat_gametime_test = X_test[X_test['type'].isin(['Assessment', 'Game', 'Activity'])]\n# #feat_gametime_test\n\n# feat_gametime_test = feat_gametime_test.groupby(['installation_id', 'game_session'], as_index=False, sort=False)['game_time'].last()\n# #feat_gametime_test\n\n# feat_gametime_test = feat_gametime_test.groupby('installation_id', as_index=False, sort=False)['game_time'].sum()\n# feat_gametime_test\n\n# # debugging\n# #X_test[X_test['installation_id'] == 'b37e2b2d']\n\n# #feat_gametime_test[feat_gametime_test['installation_id'] == 'b37e2b2d']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"len(set(X_test_gt.installation_id)), X_test_gt.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# debugging - check if df feature types\nX_test_gt.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# debugging sorting\nbooltest_sub = X_test_gt.installation_id == submission.installation_id\nset(booltest_sub)","execution_count":null,"outputs":[]},{"metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"cell_type":"code","source":"# drop installation_id\nX_test_gt = X_test_gt.drop(['installation_id'], axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(set(X_test_gt.index)), X_test_gt.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # cast forecasted assessment to str for cat_features\n# X_test_gt['forecasted_assessment'] = X_test_gt['forecasted_assessment'].astype(str)\n# type(X_test_gt.forecasted_assessment[0])\n# X_train_gt.previous_assessments_count","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Elsewise LightGBMError: Do not support special JSON characters in feature name.\nX_test_gt.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_test_gt.columns]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"list(X_test_gt.columns)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Scaler"},{"metadata":{"trusted":true},"cell_type":"code","source":"X_test_gt_scaled = scaler.fit_transform(X_test_gt.astype(np.float64))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # Catboost clf submission\n# # Important! X_test_gt_scaled added\n# cbc_preds = cbc_model.predict(X_test_gt_scaled)\n# submission['accuracy_group'] = cbc_preds.astype(int)\n# submission.to_csv(\"submission.csv\", index = False)\n# submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# #Catboost reg submission\n# # Important! X_test_gt_scaled added\n# cbc_preds = cbc_model.predict(X_test_gt_scaled)\n# submission['accuracy_group'] = np.ceil(cbc_preds).astype(int)\n# submission.to_csv(\"submission.csv\", index = False)\n# submission.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### PCA for test set"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Apply PCA for dimension reduction\n#from sklearn.decomposition import PCA\n#pca_test = PCA(n_components=10).fit(X_test_gt)\nX_test_gt = pca.transform(X_test_gt)\nprint(sum(pca.explained_variance_ratio_))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"##### Weighting and submission"},{"metadata":{"trusted":true},"cell_type":"code","source":"# LightGBM\n\nsubmission = pd.read_csv(path + 'sample_submission.csv')\ngbm_preds = gbm_model.predict(X_test_gt)\n\nsubmission['accuracy_group'] = gbm_preds\n\nsubmission['accuracy_group_weight0'] = np.where((submission['accuracy_group'] <= 0.81387600), 0, 0)\nsubmission['accuracy_group_weight1'] = np.where((submission['accuracy_group'] > 0.81387600) & (submission['accuracy_group'] <= 1.09392700), 1, 0)\nsubmission['accuracy_group_weight2'] = np.where((submission['accuracy_group'] > 1.09392700) & (submission['accuracy_group'] <= 1.42779600), 2, 0)\nsubmission['accuracy_group_weight3'] = np.where((submission['accuracy_group'] > 1.42779600), 3, 0)\nsubmission['accuracy_group'] = submission['accuracy_group_weight0'] + submission['accuracy_group_weight1'] + submission['accuracy_group_weight2'] + submission['accuracy_group_weight3']\nsubmission = submission.drop(['accuracy_group_weight0', 'accuracy_group_weight1', 'accuracy_group_weight2', 'accuracy_group_weight3'], axis=1)\n\nsubmission.to_csv(\"submission.csv\", index = False)\n\nsubmission.accuracy_group.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission.accuracy_group.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # xgboost submission\n# xgb_preds = xgb_model.predict(X_test_gt)\n# submission['accuracy_group'] = xgb_preds.astype(int)\n# submission.to_csv(\"submission.csv\", index = False)\n# submission.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# LightGBM submission\n# gbm_preds = gbm_model.predict(X_test_gt)\n# submission['accuracy_group'] = np.round(gbm_preds).astype(int)\n# submission.to_csv(\"submission.csv\", index = False)\n# submission.head()\n# submission.accuracy_group.value_counts()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"name":"DSB2019_orig_v5_light_loading.ipynb","provenance":[],"machine_shape":"hm"}},"nbformat":4,"nbformat_minor":1}